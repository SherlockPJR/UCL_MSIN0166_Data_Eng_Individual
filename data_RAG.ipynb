{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8701969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "from pydantic import Field\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72904874",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25c44e",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the daily summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4777a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the structured data\n",
    "unified_df = pd.read_parquet(\"unified_dataset.parquet\")\n",
    "# Make sure the 'date' column is in a friendly format\n",
    "unified_df['date'] = pd.to_datetime(unified_df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e36e26",
   "metadata": {},
   "source": [
    "Load the FAISS index and metadata from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5deaf997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/4zsvb3wj0z165wt09p_yp27r0000gn/T/ipykernel_16385/2049930826.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-trained Embedding Model (same as used for index)\n",
    "# Used for similarity search API compatibility — not for recomputing\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load FAISS Index\n",
    "faiss_index = faiss.read_index(\"faiss_index.index\")\n",
    "\n",
    "# Load Metadata and Construct LangChain Documents\n",
    "with open(\"faiss_metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "docs = []\n",
    "docstore_dict = {}\n",
    "index_to_docstore_id = {}\n",
    "\n",
    "for i, md in enumerate(metadata):\n",
    "    text = md.get(\"text_chunk\") or md.get(\"content\") or \"\"\n",
    "    doc = Document(page_content=text, metadata=md)\n",
    "    doc_id = str(i)\n",
    "    docs.append(doc)\n",
    "    docstore_dict[doc_id] = doc\n",
    "    index_to_docstore_id[i] = doc_id\n",
    "\n",
    "# Build the In-Memory Docstore\n",
    "docstore = InMemoryDocstore(docstore_dict)\n",
    "\n",
    "# Rebuild the FAISS Vector Store\n",
    "faiss_store = FAISS(\n",
    "    index=faiss_index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e48807",
   "metadata": {},
   "source": [
    "## 2. Create a Retriever\n",
    "\n",
    "Convert the FAISS vector store into a retriever that, given a user query, will return the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3b9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structured_summary_for_date(df, target_date_str):\n",
    "    \"\"\"\n",
    "    Return structured summary for a specific date if available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_date = pd.to_datetime(target_date_str).date()\n",
    "    except:\n",
    "        return \"Unable to parse date from input.\"\n",
    "\n",
    "    # Filter DataFrame\n",
    "    matched_row = df[df['date'] == pd.to_datetime(target_date)]\n",
    "\n",
    "    if matched_row.empty:\n",
    "        return f\"No structured metrics found for {target_date}.\"\n",
    "\n",
    "    row = matched_row.iloc[0]\n",
    "    summary = f\"Structured Metrics on {target_date}:\\n\"\n",
    "    summary += f\" - Stock: Open = {row['stock_open']}, Close = {row['stock_close']}, Volume = {row['stock_volume']}\\n\"\n",
    "    summary += f\" - Reviews: {row['num_reviews']} reviews, Avg Playtime = {row['avg_playtime_hours']:.2f} hrs, % Positive = {row['percent_positive']*100:.1f}%\\n\"\n",
    "    summary += f\" - Reddit: {row['num_reddit_posts']} posts (avg score = {row['avg_reddit_score']}), {row['num_reddit_comments']} comments\\n\"\n",
    "    summary += f\" - News: {row['num_news_articles']} articles\\n\"\n",
    "    return summary\n",
    "\n",
    "\n",
    "def semantic_search(query, k=5):\n",
    "    \"\"\"\n",
    "    Given a text query, this function:\n",
    "      - Embeds the query using SentenceTransformer\n",
    "      - Searches the FAISS index\n",
    "      - Returns the top-k chunks along with distances and metadata\n",
    "    \"\"\"\n",
    "    query_embedding = semantic_model.encode(query).astype(\"float32\").reshape(1, -1)\n",
    "    \n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        record = metadata[idx]\n",
    "        record[\"distance\"] = float(dist)\n",
    "        results.append(record)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_date_from_question(question):\n",
    "    \"\"\"\n",
    "    Extract the first valid date string from the user question using regex and dateutil.\n",
    "    \"\"\"\n",
    "    date_candidates = re.findall(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b|\\b\\w+ \\d{1,2}, \\d{4}\\b\", question)\n",
    "    for d in date_candidates:\n",
    "        try:\n",
    "            parsed = parser.parse(d)\n",
    "            return str(parsed.date())\n",
    "        except:\n",
    "            continue\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c70002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/4zsvb3wj0z165wt09p_yp27r0000gn/T/ipykernel_16385/738416381.py:1: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class CombinedRetriever(BaseRetriever):\n",
      "/var/folders/n7/4zsvb3wj0z165wt09p_yp27r0000gn/T/ipykernel_16385/738416381.py:1: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class CombinedRetriever(BaseRetriever):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "class CombinedRetriever(BaseRetriever):\n",
    "    df: any = Field(...)  # structured DataFrame\n",
    "    k: int = Field(default=5)  # top-k for semantic search\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Extract structured summary\n",
    "        date_str = extract_date_from_question(query)\n",
    "        structured_summary = (\n",
    "            get_structured_summary_for_date(self.df, date_str)\n",
    "            if date_str else get_latest_structured_summary(self.df)\n",
    "        )\n",
    "        structured_doc = Document(\n",
    "            page_content=structured_summary,\n",
    "            metadata={\"source\": \"structured_summary\"}\n",
    "        )\n",
    "\n",
    "        # Run semantic search (unstructured retrieval)\n",
    "        faiss_results = semantic_search(query, self.k)\n",
    "        unstructured_docs = [\n",
    "            Document(page_content=r.get(\"text_chunk\", r.get(\"content\", \"\")), metadata=r)\n",
    "            for r in faiss_results\n",
    "        ]\n",
    "\n",
    "        return [structured_doc] + unstructured_docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async not implemented yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c354b1f",
   "metadata": {},
   "source": [
    "## 3. Define the Prompt Template\n",
    "Build a prompt template that instructs the LLM to use the retrieved context to answer the question. You can customize this prompt to include guidelines, a fixed format, or even further instructions (such as including metadata details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e10619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful Ubisoft data assistant. \n",
    "Use the following CONTEXT to answer the question below. \n",
    "If you do not find sufficient information from CONTEXT, say you don't know.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d18ac3",
   "metadata": {},
   "source": [
    "## 4. Initialise The LLM\n",
    "Set up a large language model that will generate answers based on the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API key from .env\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialise the LLM\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3dcf4",
   "metadata": {},
   "source": [
    "## 5. Assemble the RetrievalQA Chain.\n",
    "Use LangChain’s built-in RetrievalQA chain (or a custom chain) to combine retrieval and generation. This chain will take a user query, retrieve relevant documents, inject them into the prompt, and then pass the prompt to the LLM for the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b690dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom retriever instance\n",
    "combined_retriever = CombinedRetriever(df=unified_df, k=5)\n",
    "\n",
    "# Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=combined_retriever,\n",
    "    chain_type=\"stuff\",  # 'stuff' is great if the combined context is under the token limit\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1673663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " On April 02, 2025, the stock volume was 524,163.0, and the review sentiment was 85.3% positive based on 143 reviews.\n",
      "\n",
      "Source Documents:\n",
      "\n",
      "- Source: structured_summary -> Structured Metrics on 2025-04-02:\n",
      " - Stock: Open = 10.694999694824219, Close = 10.704999923706055, Volume = 524163.0\n",
      " - Reviews: 143.0 reviews, Avg Playtime = 41.54 hrs, % Positive = 85.3%\n",
      " - Reddit: ...\n",
      "\n",
      "- Source: steam_review -> so far i'm having a great time. not sure how much stock you should put into any bad reviews because barely half the people on steam have even finished the prologue as of 3/21. game is running great on...\n",
      "\n",
      "- Source: steam_review -> ubisoft everyone would be losing their minds. what a good reminder for all of us to not put so much stock into pre-release reviews. they are trying to make money, too....\n",
      "\n",
      "- Source: reddit_comment -> you think ubisoft’s stock was stellar before march the 20th and ac shadows crashed it?...\n",
      "\n",
      "- Source: reddit_comment -> this is the most braindead take i’ve read today. if you think the stock is going down because people hate the game, you’re either gullible and can’t form your own opinion, or you’re a troll. in either...\n",
      "\n",
      "- Source: reddit_comment -> i thought so, too. a far cry from where sentiment seemed to be on release....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What was the stock volume and review sentiment on April 02, 2025?\"\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"Answer:\\n\", response[\"result\"])\n",
    "print(\"\\nSource Documents:\\n\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(f\"- Source: {doc.metadata.get('source', 'unstructured')} -> {doc.page_content[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6644305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
