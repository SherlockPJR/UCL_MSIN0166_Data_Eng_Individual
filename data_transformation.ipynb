{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93f3fc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sherlockpi/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from functools import reduce\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, count, avg, sum as spark_sum, when\n",
    "from pyspark.sql.functions import lit, to_date, col, concat_ws\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon') # Download vader_lexicon\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9420303b",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f21be7",
   "metadata": {},
   "source": [
    "## 1. Load into Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cf0d9",
   "metadata": {},
   "source": [
    "Start a Spark session for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cee63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/06 15:24:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UbisoftDataTransform\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad10e2ac",
   "metadata": {},
   "source": [
    "Load the various parquet files into Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ece9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock           = spark.read.parquet(\"data/ubisoft_stock.parquet\")\n",
    "df_news            = spark.read.parquet(\"data/ubisoft_news.parquet\")\n",
    "df_steam_reviews   = spark.read.parquet(\"data/steam_reviews.parquet\")\n",
    "df_reddit_posts    = spark.read.parquet(\"data/acshadows_reddit_posts.parquet\")\n",
    "df_reddit_comments = spark.read.parquet(\"data/acshadows_reddit_comments.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e4034",
   "metadata": {},
   "source": [
    "## 2. Fact Table: daily_summary\n",
    "\n",
    "Standardise all datetime columns to join and aggregate by date later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf78088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = df_stock.withColumn(\"date\", to_date(col(\"Date\")))  \n",
    "\n",
    "# For df_news: convert the 'date' column (currently string) to a date type.\n",
    "df_news = df_news.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# For df_steam_reviews: convert review_date to a date.\n",
    "df_steam_reviews = df_steam_reviews.withColumn(\"date\", to_date(col(\"review_date\")))\n",
    "\n",
    "# For df_reddit_posts: convert created_date to a date.\n",
    "df_reddit_posts = df_reddit_posts.withColumn(\"date\", to_date(col(\"created_date\")))\n",
    "\n",
    "# For df_reddit_comments: convert comment_created_date to a date.\n",
    "df_reddit_comments = df_reddit_comments.withColumn(\"date\", to_date(col(\"comment_created_date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eaaf2f",
   "metadata": {},
   "source": [
    "Aggregate each data source by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea379f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Stock Data\n",
    "df_stock_daily = df_stock.select(\"date\", \"Open\", \"Close\", \"Volume\")\n",
    "\n",
    "# Aggregate Steam Reviews: count reviews, average playtime, and calculate % positive reviews\n",
    "df_reviews_daily = df_steam_reviews.groupBy(\"date\").agg(\n",
    "    count(\"*\").alias(\"num_reviews\"),\n",
    "    avg(\"playtime_hours\").alias(\"avg_playtime_hours\"),\n",
    "    (spark_sum(when(col(\"voted_up\") == True, 1).otherwise(0)) / count(\"*\")).alias(\"percent_positive\")\n",
    ")\n",
    "\n",
    "# Aggregate Reddit Posts: count posts, and average score\n",
    "df_reddit_posts_daily = df_reddit_posts.groupBy(\"date\").agg(\n",
    "    count(\"*\").alias(\"num_reddit_posts\"),\n",
    "    avg(\"score\").alias(\"avg_reddit_score\")\n",
    ")\n",
    "\n",
    "# Aggregate Reddit Comments: count daily comments\n",
    "df_reddit_comments_daily = df_reddit_comments.groupBy(\"date\").agg(\n",
    "    count(\"*\").alias(\"num_reddit_comments\")\n",
    ")\n",
    "\n",
    "# News â€“ count the number of news articles per day\n",
    "df_news_daily = df_news.groupBy(\"date\").agg(\n",
    "    count(\"*\").alias(\"num_news_articles\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb5309",
   "metadata": {},
   "source": [
    "Join DataFrames: join all these daily aggregates on the common key (date) to produce a unified view (the fact table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0826c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dataframes to join (using a full outer join to include all dates)\n",
    "dfs = [df_stock_daily, df_reviews_daily, df_reddit_posts_daily, df_reddit_comments_daily, df_news_daily]\n",
    "\n",
    "# Reduce the list with successive joins on the \"date\" column\n",
    "df_unified = reduce(lambda df1, df2: df1.join(df2, on=\"date\", how=\"full\"), dfs)\n",
    "\n",
    "# Select and rename columns to match the designed schema\n",
    "df_unified = df_unified.select(\n",
    "    \"date\",\n",
    "    col(\"Open\").alias(\"stock_open\"),\n",
    "    col(\"Close\").alias(\"stock_close\"),\n",
    "    col(\"Volume\").alias(\"stock_volume\"),\n",
    "    \"num_reviews\",\n",
    "    \"avg_playtime_hours\",\n",
    "    \"percent_positive\",\n",
    "    \"num_reddit_posts\",\n",
    "    \"avg_reddit_score\",\n",
    "    \"num_reddit_comments\",\n",
    "    \"num_news_articles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42fe0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------+-----------+------------------+------------------+----------------+------------------+-------------------+-----------------+\n",
      "|date      |stock_open        |stock_close       |stock_volume|num_reviews|avg_playtime_hours|percent_positive  |num_reddit_posts|avg_reddit_score  |num_reddit_comments|num_news_articles|\n",
      "+----------+------------------+------------------+------------+-----------+------------------+------------------+----------------+------------------+-------------------+-----------------+\n",
      "|2025-04-05|NULL              |NULL              |NULL        |34         |38.470588235294116|0.7647058823529411|NULL            |NULL              |103                |NULL             |\n",
      "|2025-04-04|10.194999694824219|9.54800033569336  |1626113     |120        |38.11424999999999 |0.825             |NULL            |NULL              |601                |NULL             |\n",
      "|2025-04-03|10.399999618530273|10.345000267028809|765150      |123        |43.27024390243901 |0.7967479674796748|260             |21.08076923076923 |2187               |NULL             |\n",
      "|2025-04-02|10.694999694824219|10.704999923706055|524163      |143        |41.539300699300696|0.8531468531468531|271             |32.313653136531364|2287               |1                |\n",
      "|2025-04-01|11.1899995803833  |10.805000305175781|1406544     |186        |39.018548387096786|0.8172043010752689|75              |86.36             |1536               |NULL             |\n",
      "|2025-03-31|12.109999656677246|11.154999732971191|2431051     |227        |35.530132158590305|0.7973568281938326|8               |507.0             |456                |NULL             |\n",
      "|2025-03-30|NULL              |NULL              |NULL        |241        |38.927136929460566|0.8381742738589212|1               |219.0             |28                 |NULL             |\n",
      "|2025-03-29|NULL              |NULL              |NULL        |278        |41.60233812949637 |0.8237410071942446|1               |2908.0            |74                 |NULL             |\n",
      "|2025-03-28|14.149999618530273|12.6850004196167  |3938137     |283        |39.27522968197883 |0.7809187279151943|1               |5.0               |31                 |NULL             |\n",
      "|2025-03-27|13.0              |12.920000076293945|763231      |324        |39.19895061728396 |0.8179012345679012|NULL            |NULL              |22                 |1                |\n",
      "+----------+------------------+------------------+------------+-----------+------------------+------------------+----------------+------------------+-------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- stock_open: double (nullable = true)\n",
      " |-- stock_close: double (nullable = true)\n",
      " |-- stock_volume: long (nullable = true)\n",
      " |-- num_reviews: long (nullable = true)\n",
      " |-- avg_playtime_hours: double (nullable = true)\n",
      " |-- percent_positive: double (nullable = true)\n",
      " |-- num_reddit_posts: long (nullable = true)\n",
      " |-- avg_reddit_score: double (nullable = true)\n",
      " |-- num_reddit_comments: long (nullable = true)\n",
      " |-- num_news_articles: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 rows of the unified dataset\n",
    "df_unified.orderBy(col(\"date\").desc()).show(10, truncate=False)\n",
    "df_unified.printSchema()\n",
    "\n",
    "# Write the unified dataset to a parquet file for further analysis\n",
    "df_unified.write.mode(\"overwrite\").parquet(\"data/unified_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffd887",
   "metadata": {},
   "source": [
    "## 3. Text Table: textual_context (for NLP / RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f27618",
   "metadata": {},
   "source": [
    "Create a unified text table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0ae59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steam Reviews\n",
    "df_steam_text = df_steam_reviews.select(\n",
    "    to_date(col(\"review_date\")).alias(\"date\"),\n",
    "    lit(\"steam_review\").alias(\"source\"),\n",
    "    col(\"review\").alias(\"content\"),\n",
    "    col(\"review_id\").alias(\"id\"),\n",
    "    lit(None).cast(\"string\").alias(\"url\")\n",
    ").where(col(\"review\").isNotNull())\n",
    "\n",
    "# Reddit Posts\n",
    "df_reddit_posts_text = df_reddit_posts.select(\n",
    "    to_date(col(\"created_date\")).alias(\"date\"),\n",
    "    lit(\"reddit_post\").alias(\"source\"),\n",
    "    concat_ws(\"\\n\", col(\"title\"), col(\"selftext\")).alias(\"content\"),  # combine title + body\n",
    "    col(\"id\").alias(\"id\"),\n",
    "    col(\"url\")\n",
    ").where(col(\"title\").isNotNull() | col(\"selftext\").isNotNull())\n",
    "\n",
    "# Reddit Comments\n",
    "df_reddit_comments_text = df_reddit_comments.select(\n",
    "    to_date(col(\"comment_created_date\")).alias(\"date\"),\n",
    "    lit(\"reddit_comment\").alias(\"source\"),\n",
    "    col(\"comment_body\").alias(\"content\"),\n",
    "    col(\"comment_id\").alias(\"id\"),\n",
    "    lit(None).cast(\"string\").alias(\"url\")\n",
    ").where(col(\"comment_body\").isNotNull())\n",
    "\n",
    "# Ubisoft News\n",
    "df_news_text = df_news.select(\n",
    "    to_date(col(\"date\")).alias(\"date\"),\n",
    "    lit(\"ubisoft_news\").alias(\"source\"),\n",
    "    col(\"headline\").alias(\"content\"),\n",
    "    col(\"headline\").alias(\"id\"),  # using headline as ID (or hash if you prefer)\n",
    "    lit(\"https://news.ubisoft.com/en-gb/\").alias(\"url\")\n",
    ").where(col(\"headline\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------------------------------------------------------------------+---------+----+\n",
      "|      date|      source|                                                                         content|       id| url|\n",
      "+----------+------------+--------------------------------------------------------------------------------+---------+----+\n",
      "|2025-04-05|steam_review|                                                                        Its fun.|192017461|NULL|\n",
      "|2025-04-05|steam_review|                                                    \"same shit different toilet\"|192017163|NULL|\n",
      "|2025-04-05|steam_review|I was a huge ac fan all the way back from AC 2. But some how starting from th...|192017026|NULL|\n",
      "|2025-04-05|steam_review|very good, Assassins creed in Japan has been a long awaited installment in th...|192016829|NULL|\n",
      "|2025-04-05|steam_review|                                                                 a piece of shit|192016602|NULL|\n",
      "+----------+------------+--------------------------------------------------------------------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- source: string (nullable = false)\n",
      " |-- content: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union all\n",
    "df_textual_context = df_steam_text.unionByName(df_reddit_posts_text)\\\n",
    "                                  .unionByName(df_reddit_comments_text)\\\n",
    "                                  .unionByName(df_news_text)\n",
    "\n",
    "# Preview\n",
    "df_textual_context.show(5, truncate=80)\n",
    "df_textual_context.printSchema()\\\n",
    "\n",
    "# Save to parquet\n",
    "df_textual_context.write.mode(\"overwrite\").parquet(\"data/textual_context.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fae138",
   "metadata": {},
   "source": [
    "Preprocessing (cleaning and generate embeddings) text for RAG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb5c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load  textual context into pandas df\n",
    "df = df_textual_context.toPandas()\n",
    "\n",
    "# Define helper functions for text cleaning and chunking\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize text by lowercasing and removing excessive whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=100, overlap=20):\n",
    "    \"\"\"\n",
    "    Split text into chunks of up to `chunk_size` words with an overlap.\n",
    "    Uses simple whitespace tokenization.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(words):\n",
    "            break\n",
    "        start = end - overlap  # Overlap for context continuity\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and chunk the textual content\n",
    "df['clean_content'] = df['content'].apply(lambda x: clean_text(x) if isinstance(x, str) else \"\")\n",
    "df['chunks'] = df['clean_content'].apply(lambda x: chunk_text(x, chunk_size=100, overlap=20))\n",
    "\n",
    "# Explode the list of chunks so each chunk gets its own row while preserving metadata\n",
    "df_chunks = df.explode('chunks').reset_index(drop=True)\n",
    "df_chunks.rename(columns={'chunks': 'text_chunk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12596e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each text chunk using a pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Choose a model that suits your needs\n",
    "df_chunks['embedding'] = df_chunks['text_chunk'].apply(lambda x: model.encode(x).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3dae0",
   "metadata": {},
   "source": [
    "Additional NLP feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8de7e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute TextBlob-based sentiment and objectivity scores\n",
    "def compute_textblob_sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - polarity: sentiment score between -1 (negative) and 1 (positive)\n",
    "      - subjectivity: score between 0 (objective) and 1 (subjective)\n",
    "      - objectivity: computed as 1 - subjectivity\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    objectivity = 1 - subjectivity\n",
    "    return polarity, subjectivity, objectivity\n",
    "\n",
    "# Apply the TextBlob function to compute polarity, subjectivity, and objectivity\n",
    "df_chunks[['polarity', 'subjectivity', 'objectivity']] = df_chunks['text_chunk'].apply(\n",
    "    lambda x: pd.Series(compute_textblob_sentiment(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3bc1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to compute VADER sentiment scores\n",
    "def compute_vader_scores(text):\n",
    "    \"\"\"\n",
    "    Returns a pandas Series with VADER scores:\n",
    "      - neg: Negative sentiment score\n",
    "      - neu: Neutral sentiment score\n",
    "      - pos: Positive sentiment score\n",
    "      - compound: Normalized compound score (overall sentiment)\n",
    "    \"\"\"\n",
    "    return pd.Series(sia.polarity_scores(text))\n",
    "\n",
    "# Apply VADER to each text chunk\n",
    "df_chunks[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']] = df_chunks['text_chunk'].apply(\n",
    "    compute_vader_scores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9f636b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20607 entries, 0 to 20606\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            20607 non-null  object \n",
      " 1   source          20607 non-null  object \n",
      " 2   content         20607 non-null  object \n",
      " 3   id              20607 non-null  object \n",
      " 4   url             942 non-null    object \n",
      " 5   clean_content   20607 non-null  object \n",
      " 6   text_chunk      20607 non-null  object \n",
      " 7   embedding       20607 non-null  object \n",
      " 8   polarity        20607 non-null  float64\n",
      " 9   subjectivity    20607 non-null  float64\n",
      " 10  objectivity     20607 non-null  float64\n",
      " 11  vader_neg       20607 non-null  float64\n",
      " 12  vader_neu       20607 non-null  float64\n",
      " 13  vader_pos       20607 non-null  float64\n",
      " 14  vader_compound  20607 non-null  float64\n",
      "dtypes: float64(7), object(8)\n",
      "memory usage: 2.4+ MB\n",
      "                                          text_chunk  polarity  subjectivity  \\\n",
      "0                                           its fun.  0.300000      0.200000   \n",
      "1                       \"same shit different toilet\" -0.058333      0.381250   \n",
      "2  i was a huge ac fan all the way back from ac 2... -0.062155      0.400125   \n",
      "3  story line and the acting are generic, even un...  0.054167      0.220833   \n",
      "4  very good, assassins creed in japan has been a...  0.430000      0.590000   \n",
      "\n",
      "   objectivity  vader_neg  vader_neu  vader_pos  vader_compound  \n",
      "0     0.800000      0.000      0.233      0.767          0.5106  \n",
      "1     0.618750      0.545      0.455      0.000         -0.5574  \n",
      "2     0.599875      0.031      0.875      0.094          0.6829  \n",
      "3     0.779167      0.061      0.763      0.176          0.6478  \n",
      "4     0.410000      0.042      0.837      0.121          0.4754  \n"
     ]
    }
   ],
   "source": [
    "# Display a preview of the DataFrame with the new sentiment features\n",
    "df_chunks.info()\n",
    "print(df_chunks[['text_chunk', 'polarity', 'subjectivity', 'objectivity', \n",
    "                 'vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c602e",
   "metadata": {},
   "source": [
    "FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4f56a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a FAISS index for efficient similarity search\n",
    "embeddings = np.array(df_chunks['embedding'].tolist()).astype(\"float32\")\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # L2 distance index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index to disk for later use\n",
    "faiss.write_index(index, \"faiss_index.index\")\n",
    "\n",
    "# Save metadata for each chunk\n",
    "metadata = df_chunks[['date', 'source', 'id', 'url', 'text_chunk']].to_dict(orient='records')\n",
    "with open(\"faiss_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbbe2f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "accelerate==1.5.2\n",
      "aiohappyeyeballs==2.6.1\n",
      "aiohttp==3.11.14\n",
      "aiosignal==1.3.2\n",
      "annotated-types==0.7.0\n",
      "anyio==4.9.0\n",
      "appnope==0.1.3\n",
      "asttokens==2.4.0\n",
      "astunparse==1.6.3\n",
      "attrs==25.3.0\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.13.3\n",
      "certifi==2023.7.22\n",
      "charset-normalizer==3.4.1\n",
      "click==8.1.8\n",
      "cloudpickle==3.1.1\n",
      "comm==0.1.4\n",
      "contourpy==1.3.1\n",
      "cycler==0.12.1\n",
      "datasets==3.4.1\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "dill==0.3.8\n",
      "distro==1.9.0\n",
      "dotenv==0.9.9\n",
      "emoji==2.14.1\n",
      "et_xmlfile==2.0.0\n",
      "executing==2.0.0\n",
      "faiss-cpu==1.10.0\n",
      "filelock==3.18.0\n",
      "flatbuffers==25.2.10\n",
      "fonttools==4.56.0\n",
      "frozendict==2.4.6\n",
      "frozenlist==1.5.0\n",
      "fsspec==2024.12.0\n",
      "gast==0.6.0\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.71.0\n",
      "h11==0.14.0\n",
      "h5py==3.13.0\n",
      "httpcore==1.0.7\n",
      "httpx==0.28.1\n",
      "huggingface-hub==0.29.3\n",
      "idna==3.10\n",
      "ipykernel==6.25.2\n",
      "ipython==8.16.1\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.6\n",
      "jiter==0.9.0\n",
      "joblib==1.4.2\n",
      "jupyter_client==8.4.0\n",
      "jupyter_core==5.4.0\n",
      "keras==3.9.0\n",
      "kiwisolver==1.4.8\n",
      "langdetect==1.0.9\n",
      "libclang==18.1.1\n",
      "llvmlite==0.44.0\n",
      "Markdown==3.7\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==3.0.2\n",
      "matplotlib==3.10.1\n",
      "matplotlib-inline==0.1.6\n",
      "mdurl==0.1.2\n",
      "ml_dtypes==0.5.1\n",
      "mpmath==1.3.0\n",
      "multidict==6.2.0\n",
      "multiprocess==0.70.16\n",
      "multitasking==0.0.11\n",
      "namex==0.0.8\n",
      "nest-asyncio==1.5.8\n",
      "networkx==3.4.2\n",
      "nltk==3.9.1\n",
      "numba==0.61.0\n",
      "numpy==2.1.3\n",
      "openai==1.68.2\n",
      "openpyxl==3.1.5\n",
      "opt_einsum==3.4.0\n",
      "optree==0.14.1\n",
      "outcome==1.3.0.post0\n",
      "packaging==23.2\n",
      "pandas==2.2.3\n",
      "parso==0.8.3\n",
      "patsy==1.0.1\n",
      "peewee==3.17.9\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "pillow==11.1.0\n",
      "platformdirs==3.11.0\n",
      "praw==7.8.1\n",
      "prawcore==2.4.0\n",
      "prompt-toolkit==3.0.39\n",
      "propcache==0.3.1\n",
      "protobuf==5.29.3\n",
      "psutil==5.9.6\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "py4j==0.10.9.7\n",
      "pyarrow==19.0.1\n",
      "pydantic==2.10.6\n",
      "pydantic_core==2.27.2\n",
      "Pygments==2.16.1\n",
      "pyparsing==3.2.1\n",
      "PySocks==1.7.1\n",
      "pyspark==3.5.5\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.1.0\n",
      "pytz==2025.1\n",
      "PyYAML==6.0.2\n",
      "pyzmq==25.1.1\n",
      "regex==2024.11.6\n",
      "requests==2.32.3\n",
      "rich==13.9.4\n",
      "safetensors==0.5.3\n",
      "scikeras==0.13.0\n",
      "scikit-learn==1.6.1\n",
      "scipy==1.15.2\n",
      "seaborn==0.13.2\n",
      "selenium==4.30.0\n",
      "sentence-transformers==4.0.2\n",
      "shap==0.47.0\n",
      "six==1.16.0\n",
      "slicer==0.0.8\n",
      "sniffio==1.3.1\n",
      "sortedcontainers==2.4.0\n",
      "soupsieve==2.6\n",
      "stack-data==0.6.3\n",
      "statsmodels==0.14.4\n",
      "steamreviews==0.9.5\n",
      "sympy==1.13.1\n",
      "tensorboard==2.19.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.19.0\n",
      "tensorflow-io-gcs-filesystem==0.37.1\n",
      "termcolor==2.5.0\n",
      "textblob==0.19.0\n",
      "tf_keras==2.19.0\n",
      "threadpoolctl==3.6.0\n",
      "tokenizers==0.21.1\n",
      "torch==2.6.0\n",
      "torchaudio==2.6.0\n",
      "torchvision==0.21.0\n",
      "tornado==6.3.3\n",
      "tqdm==4.67.1\n",
      "traitlets==5.11.2\n",
      "transformers==4.50.1\n",
      "trio==0.29.0\n",
      "trio-websocket==0.12.2\n",
      "typing_extensions==4.12.2\n",
      "tzdata==2025.1\n",
      "update-checker==0.18.0\n",
      "urllib3==2.3.0\n",
      "wcwidth==0.2.8\n",
      "websocket-client==1.8.0\n",
      "Werkzeug==3.1.3\n",
      "wordcloud==1.9.4\n",
      "wrapt==1.17.2\n",
      "wsproto==1.2.0\n",
      "xgboost==3.0.0\n",
      "xxhash==3.5.0\n",
      "yarl==1.18.3\n",
      "yfinance==0.2.55\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f2603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
